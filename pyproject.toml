
[build-system]
build-backend = "poetry.core.masonry.api"
requires = ["poetry-core"]

[tool]

[tool.black]
exclude = '''
/(
    \.eggs
  | \.git
  | \.hg
  | \.mypy_cache
  | \.tox
  | \.venv
  | _build
  | buck-out
  | build
  | dist
)/
'''
include = '\.pyi?$'
line-length = 100
target-version = ['py36', 'py37', 'py38', 'py39']

[tool.isort]
known_local_folder = [
  'e3c-llm'
]
line_length = 100
profile = 'black'

[tool.poetry]
authors = ["Simon Meoni <simonmeoni@aol.com>"]
description = "a containerized llama model"
license = "apache-2.0"
name = "llama"
readme = "README.md"
version = "0.1.0"

[tool.poetry.dependencies]
python = "~3.9"
pyllama = "^0.0.9"
uvicorn = "^0.21.1"
fastapi = "^0.95.0"
